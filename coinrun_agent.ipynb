{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Jx5dkudPV6"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "Read the assignment instructions here: https://github.com/markriedl/coinrun-game-ai-assignment\n",
        "\n",
        "Those new to pytorch may find this [primer](https://colab.research.google.com/drive/1DgkVmi6GksWOByhYVQpyUB4Rk3PUq0Cp) helpful.\n",
        "\n",
        "Those new to convolutional neural networks may find this [example](https://colab.research.google.com/drive/1740OwEI4oi5QlPtq4UFd3ha44yxef7gU) helpful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbcPfbI2CBGr"
      },
      "source": [
        "# Installation\n",
        "\n",
        "Run the following cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rodhvmBKCDI6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# del os.environ['LD_PRELOAD']\n",
        "# !apt-get remove libtcmalloc*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApGB65bRXdWL",
        "outputId": "c575c4f9-eae4-4c4a-be53-caf6889a4595"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install mpich build-essential qt5-default pkg-config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nfOUNUHYHk4",
        "outputId": "55fdb2cd-3e06-4fce-93ac-8c93b8512dc0"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/markriedl/coinrun-game-ai-assignment.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCz2c9bYYNPr",
        "outputId": "20b7c80c-852b-4592-ee1a-93478c42d81c"
      },
      "outputs": [],
      "source": [
        "!pip install -r coinrun-game-ai-assignment/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-MBLQF6MYUw0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'coinrun-game-ai-assignment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW2gBefjYZU1",
        "outputId": "9dec4e3a-7478-4756-ea1a-7a5eeb66d709"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzI7EuW1C4Dj"
      },
      "source": [
        "If the prior cell repots ```False```, then use notebooks settings to turn GPU support on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg9yJt6MYhJl",
        "outputId": "470340a0-8986-4154-8002-42befe2a9c4d"
      },
      "outputs": [],
      "source": [
        "# THIS TESTS THE COINRUN INSTALLATION\n",
        "from coinrun.random_agent import random_agent\n",
        "\n",
        "random_agent(max_steps=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJgvgUwlQ733"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rOihwhu6QDsA"
      },
      "outputs": [],
      "source": [
        "def in_ipynb():\n",
        "  try:\n",
        "    result = get_ipython().__class__.__name__\n",
        "    if 'Shell' in result:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "IN_PYNB = in_ipynb()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "w8I-vSF8QBud"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "from coinrun import setup_utils, make\n",
        "import coinrun.main_utils as utils\n",
        "from coinrun.config import Config\n",
        "if not IN_PYNB:\n",
        "    from gym.envs.classic_control import rendering\n",
        "from coinrun import policies, wrappers\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import pdb\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6OiEyfpYstU",
        "outputId": "03cbc453-d872-4fb8-b2b7-4cebf751dc44"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Resize the screen to this\n",
        "RESIZE_CONST = 40 \n",
        "\n",
        "\n",
        "# Game seed information\n",
        "NUM_LEVELS = 1 # repeat the same level over and over\n",
        "EASY_LEVEL = 1 # Start on a very small map, no enemies\n",
        "EASY_LEVEL2 = 5 # Very small map, no enemies\n",
        "MEDIUM_LEVEL = 20 # Medium length, no enemies\n",
        "MEDIUM_LEVEL2 = 45 # Medium length, no enemies\n",
        "ONE_MONSTER = 10 # Short map with one monster\n",
        "HARD_LEVEL = 7 # Longer and with monsters\n",
        "LAVA_LEVEL = 3 # Longer and with lava and pits\n",
        "\n",
        "# Defaults\n",
        "RENDER_SCREEN = False\n",
        "SAVE_FILENAME = 'saved.model'\n",
        "LOAD_FILENAME = 'saved.model'\n",
        "MODEL_PATH = 'saved_models' \n",
        "SEED = EASY_LEVEL\n",
        "\n",
        "# Don't play with this\n",
        "EVAL_EPSILON = 0.1\n",
        "EVAL_WINDOW_SIZE = 3\n",
        "EVAL_COUNT = 10\n",
        "TIMEOUT = 1000\n",
        "COIN_REWARD = 100\n",
        "\n",
        "### Data structure for holding experiences for replay\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "### Function for resizing the screen\n",
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(RESIZE_CONST, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "### Save the model. Extra information can be added to the end of the filename\n",
        "def save_model(model, filename, extras = None):\n",
        "    if extras is not None:\n",
        "        filename = filename + '.' + str(extras)\n",
        "    print(\"Saving\", filename, \"...\")\n",
        "    torch.save(model, os.path.join(MODEL_PATH, filename))\n",
        "    print(\"Done saving.\")\n",
        " \n",
        "### Load the model. If there are multiple versions with extra information at the\n",
        "### end of the filename, get the latest.\n",
        "def load_model(filename, extras = None):\n",
        "    if extras is not None:\n",
        "        filename = filename + '.' + str(extras)\n",
        "    model = None\n",
        "    candidates = [os.path.join(MODEL_PATH, f) for f in os.listdir(MODEL_PATH) if filename in f]\n",
        "    if len(candidates) > 0:\n",
        "        candidates = sorted(candidates, key=lambda f:os.stat(f).st_mtime, reverse=True)\n",
        "        filename = candidates[0]\n",
        "        print(\"Loading\", filename, \"...\")\n",
        "        model = torch.load(filename)\n",
        "        print(\"Done loading.\")\n",
        "    return model\n",
        "  \n",
        "### Give a text description of the outcome of an episode and also a score\n",
        "### Score is duration, unless the agent died.\n",
        "def episode_status(duration, reward):\n",
        "    status = \"\"\n",
        "    score = 0\n",
        "    if duration >= TIMEOUT:\n",
        "        status = \"timeout\"\n",
        "        score = duration\n",
        "    elif reward < COIN_REWARD:\n",
        "        status = \"died\"\n",
        "        score = TIMEOUT\n",
        "    else:\n",
        "        status = \"coin\"\n",
        "        score = duration\n",
        "    return status, score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s933aiIDpoL9"
      },
      "source": [
        "Every time the code runs an ```evaluate()``` of the agent's model, a new directory will be created in ```/content/cache``` and screenshots of the agent in action will be stored. Filenames are of the form ```/content/cache/eval_number/eval + screen_number + .jpeg```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "g5N9wLAepnhC"
      },
      "outputs": [],
      "source": [
        "RUN_NUM = 0\n",
        "SCREEN_SAVE = False\n",
        "SCREEN_COUNT = 0\n",
        "SCREEN_SAVE_PREFIX = 'eval'\n",
        "SCREEN_SAVE_POSTFIX = '.jpeg'\n",
        "TEMP_DIR = 'cache'\n",
        "SCREEN_SAVE_RATIO = 0.5\n",
        "\n",
        "#Screen is numpy array\n",
        "def save_screen(screen):\n",
        "  global SCREEN_COUNT\n",
        "  screen_array_t = np.transpose(screen, (1, 2, 0))\n",
        "  img = Image.fromarray(np.uint8(screen_array_t * 255))\n",
        "  width, height = img.size\n",
        "  img = img.resize((int(width*SCREEN_SAVE_RATIO), int(height*SCREEN_SAVE_RATIO)), Image.LANCZOS)\n",
        "  if not os.path.isdir(TEMP_DIR):\n",
        "    os.mkdir(TEMP_DIR)\n",
        "  if not os.path.isdir(os.path.join(TEMP_DIR, str(RUN_NUM))):\n",
        "    os.mkdir(os.path.join(TEMP_DIR, str(RUN_NUM)))\n",
        "  img.save(os.path.join(TEMP_DIR, str(RUN_NUM), SCREEN_SAVE_PREFIX + str(SCREEN_COUNT) + SCREEN_SAVE_POSTFIX), \"JPEG\")\n",
        "  SCREEN_COUNT = SCREEN_COUNT + 1\n",
        "\n",
        "def show_movie(num):\n",
        "  dir_name = str(num)\n",
        "  files = []\n",
        "  for file in os.listdir(os.path.join(TEMP_DIR, dir_name)):\n",
        "    files.append(file)\n",
        "  sorted_files = sorted(files, key=lambda f:int(f[len(SCREEN_SAVE_PREFIX):-len(SCREEN_SAVE_POSTFIX)]))\n",
        "  for file in sorted_files:\n",
        "    img = Image.open(os.path.join(TEMP_DIR, dir_name, file), 'r')\n",
        "    plt.imshow(img)\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "\n",
        "def make_anim(num):\n",
        "  dir_name = str(num)\n",
        "  files = []\n",
        "  images = []\n",
        "  for file in os.listdir(os.path.join(TEMP_DIR, dir_name)):\n",
        "    files.append(file)\n",
        "  sorted_files = sorted(files, key=lambda f:int(f[len(SCREEN_SAVE_PREFIX):-len(SCREEN_SAVE_POSTFIX)]))\n",
        "  for file in sorted_files:\n",
        "    try:\n",
        "      img = Image.open(os.path.join(TEMP_DIR, dir_name, file))\n",
        "      images.append(img)\n",
        "    except:\n",
        "      print(os.path.join(TEMP_DIR, dir_name, file), \"did not load.\")\n",
        "  images[0].save('movie' + str(num) + '.gif', \"GIF\",\n",
        "                      save_all=True,\n",
        "                      append_images=images[1:],\n",
        "                      duration=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDC4SQYzRAY8"
      },
      "source": [
        "# Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "B930j8EMQq2l"
      },
      "outputs": [],
      "source": [
        "\n",
        "BATCH_SIZE = 128            # How many replay experiences to run through neural net at once\n",
        "GAMMA = 0.999               # How much to discount the future [0..1]\n",
        "BOOTSTRAP = 20000           # How many steps to run to fill up replay memory before training starts\n",
        "TARGET_UPDATE = 0           # Delays updating the network for loss calculations. 0=don't delay, or 1+ number of episodes\n",
        "REPLAY_CAPACITY = 10000     # How big is the replay memory\n",
        "EPSILON = 0.9               # Use random action if less than epsilon [0..1]\n",
        "EVAL_INTERVAL = 10          # How many episodes of training before evaluation\n",
        "NUM_EPISODES = 200          # Max number of training episodes\n",
        "RANDOM_SEED = None          # Seed for random number generator, for reproducability, use None for random seed\n",
        "EPSILON_DECAY_NUM = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGXg2Wp0Q2m5"
      },
      "source": [
        "# Reference Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcBRVLjwUIl6"
      },
      "source": [
        "## Unit Testing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "Y_McIDrJPgnT"
      },
      "outputs": [],
      "source": [
        "\n",
        "def testReplayMemory():\n",
        "    print(\"Testing ReplayMemory...\")\n",
        "    capacity = 100\n",
        "    test_replay_memory = ReplayMemory(capacity)\n",
        "    for i in range(capacity):\n",
        "        test_replay_memory.push(i, i, i, i)\n",
        "    assert (len(test_replay_memory) == capacity),\"size test failed\"\n",
        "    for i in range(len(test_replay_memory)):\n",
        "        item = test_replay_memory.memory[i]\n",
        "        assert (item[0] == i), \"item\" + str(i) + \"not holding the correct value\"\n",
        "    for i in range(capacity//2):\n",
        "        test_replay_memory.push(capacity+i, capacity+i, capacity+i, capacity+i)\n",
        "    assert (len(test_replay_memory) == capacity), \"size test 2 failed\"\n",
        "    # check items\n",
        "    for i in range(len(test_replay_memory)):\n",
        "        item = test_replay_memory.memory[i]\n",
        "        if i < capacity // 2:\n",
        "            assert (item[0] == i+capacity), \"not holding the correct value after looping (first half)\"\n",
        "        else:\n",
        "            assert (item[0] == i), \"not holding the correct value after looping (second half)\"\n",
        "    print(\"ReplayMemory test passed.\")\n",
        "    return True\n",
        "  \n",
        "def testMakeBatch():\n",
        "    print(\"Testing doMakeBatch...\")\n",
        "    batch_size = 128\n",
        "    capacity = batch_size * 2\n",
        "    test_replay_memory = ReplayMemory(capacity)\n",
        "    state = None\n",
        "    new_state = None\n",
        "    action = None\n",
        "    reward = None\n",
        "    # Test types and shapes of return values\n",
        "    for i in range(capacity):\n",
        "        state = torch.randn(1, 3, 80, 80, device=DEVICE)\n",
        "        new_state = torch.randn(1, 3, 80, 80, device=DEVICE)\n",
        "        action = torch.randn(1, 1, device=DEVICE)\n",
        "        reward = torch.randn(1, 1, device=DEVICE)\n",
        "        test_replay_memory.push(state, action, new_state, reward)\n",
        "    states_batch, actions_batch, next_states_batch, rewards_batch, non_final_mask = doMakeBatch(test_replay_memory, batch_size)\n",
        "    assert(type(states_batch) == torch.Tensor and states_batch.size() == (batch_size, 3, 80, 80)), \"states batch not correct shape.\"\n",
        "    assert(type(actions_batch) == torch.Tensor and actions_batch.size() == (batch_size, 1)), \"actions batch not correct shape.\"\n",
        "    assert(type(next_states_batch) == torch.Tensor and next_states_batch.size() == (batch_size, 3, 80, 80)), \"next states batch not correct shape.\"\n",
        "    assert(type(rewards_batch) == torch.Tensor and rewards_batch.size() == (batch_size, 1)), \"rewards batch not correct shape.\"\n",
        "    assert(type(non_final_mask) == type(torch.tensor(batch_size, dtype=torch.bool, device=DEVICE)) and non_final_mask.size()[0] == batch_size), \"non-final mask not correct shape.\"\n",
        "\n",
        "    # Test mask\n",
        "    test_replay_memory = ReplayMemory(batch_size)\n",
        "    for i in range(batch_size):\n",
        "        state = torch.randn(1, 3, 80, 80, device=DEVICE)\n",
        "        new_state = None\n",
        "        if i % 2 == 0:\n",
        "            new_state = torch.randn(1, 3, 80, 80, device=DEVICE)\n",
        "        action = torch.randn(1, 1, device=DEVICE)\n",
        "        reward = torch.randn(1, 1, device=DEVICE)\n",
        "        test_replay_memory.push(state, action, new_state, reward)\n",
        "    states_batch, actions_batch, next_states_batch, rewards_batch, non_final_mask = doMakeBatch(test_replay_memory, batch_size)\n",
        "    assert(non_final_mask.sum() == batch_size//2), \"non_final_mask not masking properly.\"\n",
        "    print(\"doMakeBatch test passed.\")\n",
        "    return True\n",
        "\n",
        "class UnitTestDQN(nn.Module):\n",
        "    def __init__(self, h, w, num_actions):\n",
        "        super(UnitTestDQN, self).__init__()\n",
        "        self.num_actions = num_actions\n",
        "    def forward(self, x):\n",
        "        assert(False), \"Network should not be queried when epsilon = 1.0.\" \n",
        "        return None\n",
        "\n",
        "def testSelectAction():\n",
        "    print(\"Testing select_action...\")\n",
        "    from scipy.stats import chisquare\n",
        "    sample_size = 10000\n",
        "    num_tests = 100\n",
        "    pass_rate = 0.9\n",
        "    screen_height = 40\n",
        "    screen_width = 40\n",
        "    epsilon = 1.0\n",
        "    num_actions = 7\n",
        "    test_results = {True: 0, False: 0}\n",
        "    significance_level = 0.02\n",
        "    net = UnitTestDQN(screen_height, screen_width, num_actions).to(DEVICE)\n",
        "    state = torch.randn(1, 3, 80, 80, device=DEVICE)\n",
        "    for j in range(num_tests):\n",
        "        samples = {}\n",
        "        for i in range(sample_size):\n",
        "            action, new_epsilon = select_action(state, net, num_actions, epsilon, steps_done = 0, bootstrap_threshold = 2)\n",
        "            assert(type(action) == torch.Tensor and action.size() == (1,1)), \"Action not correct shape.\"\n",
        "            assert(new_epsilon == epsilon), \"Epsilon should not change during bootstrapping.\"\n",
        "            action = action.item()\n",
        "            if action not in samples:\n",
        "                samples[action] = 0\n",
        "            samples[action] = samples[action] + 1\n",
        "        expected = [sample_size / num_actions] * num_actions\n",
        "        statistic, pvalue = chisquare(f_obs=list(samples.values()), f_exp=expected)\n",
        "        test_results[pvalue >= significance_level] += 1\n",
        "    assert(test_results[True] > pass_rate * num_tests), \"Random sample is not from uniform distribution.\"    \n",
        "    print(\"select_action test passed.\")\n",
        "    return True\n",
        "\n",
        "def testPredictQValues():\n",
        "    print(\"Testing doPredictQValues...\")\n",
        "    batch_size = 128\n",
        "    screen_height = 80\n",
        "    screen_width = 80\n",
        "    num_actions = 7\n",
        "    net = DQN(screen_height, screen_width, num_actions).to(DEVICE)\n",
        "    states_batch = torch.randn(batch_size, 3, 80, 80, device=DEVICE)\n",
        "    actions_batch = torch.randint(0, 7, (128, 1), device=DEVICE).long()\n",
        "    state_action_values = doPredictQValues(net, states_batch, actions_batch)\n",
        "    assert(type(state_action_values) == torch.Tensor and state_action_values.size() == (128, 1)), \"Return value not correct shape.\"\n",
        "    print(\"doPredictQValues test passed.\")\n",
        "    return True\n",
        "\n",
        "def testPredictNextStateUtilities():\n",
        "    print(\"Testing doPredictNextStateUtilities...\")\n",
        "    screen_height = 80\n",
        "    screen_width = 80\n",
        "    num_actions = 7\n",
        "    batch_size = 128\n",
        "    passed = False\n",
        "    net = DQN(screen_height, screen_width, num_actions).to(DEVICE)\n",
        "    # First option to try is that the batch is full sized.\n",
        "\n",
        "    next_states_batch = torch.ones(batch_size, 3, 80, 80, device=DEVICE)\n",
        "    non_final_mask = torch.ones(batch_size, dtype=torch.bool, device=DEVICE)\n",
        "    for i in range(batch_size):\n",
        "        if i % 2 == 1:\n",
        "            next_states_batch[i].fill_(0)\n",
        "            non_final_mask[i] = 0\n",
        "    next_state_values = doPredictNextStateUtilities(net, next_states_batch, non_final_mask, batch_size)\n",
        "    assert(type(next_state_values) == torch.Tensor and next_state_values.size() == (batch_size, 1)), \"Return value not correct shape (attempt 1).\"\n",
        "    for i in range(batch_size):\n",
        "        if i % 2 == 1:\n",
        "            assert(next_state_values[i].sum() == 0), \"Element \" + str(i) + \"is not 0.0 when non_final_mask[i] = 0\"\n",
        "    passed = True\n",
        "\n",
        "    try:\n",
        "        next_states_batch = torch.ones(batch_size, 3, 80, 80, device=DEVICE)\n",
        "        non_final_mask = torch.ones(batch_size, dtype=torch.bool, device=DEVICE)\n",
        "        for i in range(batch_size):\n",
        "            if i % 2 == 1:\n",
        "                next_states_batch[i].fill_(0)\n",
        "                non_final_mask[i] = 0\n",
        "        next_state_values = doPredictNextStateUtilities(net, next_states_batch, non_final_mask, batch_size)\n",
        "        assert(type(next_state_values) == torch.Tensor and next_state_values.size() == (batch_size, 1)), \"Return value not correct shape (attempt 1).\"\n",
        "        for i in range(batch_size):\n",
        "            if i % 2 == 1:\n",
        "                assert(next_state_values[i].sum() == 0), \"Element \" + str(i) + \"is not 0.0 when non_final_mask[i] = 0\"\n",
        "        passed = True\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "        print(\"Will try alternative test.\")\n",
        "    if not passed:\n",
        "        # Next option is that batch is not full sized.\n",
        "        try:\n",
        "            next_states_batch = torch.ones(batch_size-1, 3, 80, 80, device=DEVICE)\n",
        "            non_final_mask = torch.ones(batch_size, dtype=torch.bool, device=DEVICE)\n",
        "            non_final_mask[0] = 0\n",
        "            next_state_values = doPredictNextStateUtilities(net, next_states_batch, non_final_mask, batch_size)\n",
        "            assert(type(next_state_values) == torch.Tensor and next_state_values.size() == (batch_size, 1)), \"Return value not correctd shape (attempt 2).\"\n",
        "            passed = True\n",
        "        except RuntimeError as e:\n",
        "            print(e)\n",
        "            print(\"No further alternative tests available.\")\n",
        "    if passed:\n",
        "        print(\"doPredictNextStateUtilities test passed.\")\n",
        "        return True\n",
        "    assert(False), \"doPredictNextStateUtilities did NOT pass test.\"\n",
        "\n",
        "def testComputeExpectedQValues():\n",
        "    print(\"Testing doComputeExpectedQValues...\")\n",
        "    batch_size = 128\n",
        "    gamma = 0.5\n",
        "    next_state_values = torch.ones(batch_size).unsqueeze(1)\n",
        "    rewards_batch = torch.ones(batch_size).unsqueeze(1)\n",
        "    expected_state_action_values = doComputeExpectedQValues(next_state_values, rewards_batch, gamma)\n",
        "    assert(type(expected_state_action_values) == torch.Tensor and expected_state_action_values.size()[0] == batch_size), \"Return value not expected shape.\"\n",
        "    for i in range(batch_size):\n",
        "        assert(expected_state_action_values[i].item() == 1.5), \"Element \" + str(i) + \" doesn't have the correct value.\"\n",
        "    print(\"doComputeExpectedQValues test passed.\")\n",
        "    return True\n",
        "\n",
        "def testComputeLoss():\n",
        "    print(\"Testing doComputeLoss...\")\n",
        "    batch_size = 128\n",
        "    state_action_values = torch.randn(batch_size, device=DEVICE)\n",
        "    expected_state_action_values = torch.randn(batch_size, device=DEVICE)\n",
        "    loss = doComputeLoss(state_action_values, expected_state_action_values)\n",
        "    assert(type(loss) == torch.Tensor and len(loss.size()) == 0), \"Loss not of expected shape.\"\n",
        "    print(\"doComputeLoss test passed.\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def unit_test():\n",
        "    testReplayMemory()\n",
        "    testMakeBatch()\n",
        "    #testSelectAction()\n",
        "    testPredictQValues()\n",
        "    testPredictNextStateUtilities()\n",
        "    testComputeExpectedQValues()\n",
        "    testComputeLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpQNNe2yUN_d"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "nPuXQWPDQTem"
      },
      "outputs": [],
      "source": [
        "\n",
        "### Training loop.\n",
        "### Each episode is a game that runs until the agent gets the coin or the game times out.\n",
        "### Train for a given number of episodes.\n",
        "def train(num_episodes = NUM_EPISODES, load_filename = None, save_filename = None, eval_interval = EVAL_INTERVAL, replay_capacity = REPLAY_CAPACITY, bootstrap_threshold = BOOTSTRAP, epsilon = EPSILON, eval_epsilon = EVAL_EPSILON, gamma = GAMMA, batch_size = BATCH_SIZE, target_update = TARGET_UPDATE, random_seed = RANDOM_SEED, num_levels = NUM_LEVELS, seed = SEED):\n",
        "    # Set the random seed\n",
        "    if random_seed is not None:\n",
        "        random.seed(random_seed)\n",
        "        torch.manual_seed(random_seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "    # Set up the environment\n",
        "    setup_utils.setup_and_load(use_cmd_line_args=False, is_high_res=True, num_levels=num_levels, set_seed=seed)\n",
        "    env = make('standard', num_envs=1)\n",
        "    if RENDER_SCREEN and not IN_PYNB:\n",
        "        env.render()\n",
        "\n",
        "    # Reset the environment\n",
        "    env.reset()\n",
        "\n",
        "    # Get screen size so that we can initialize layers correctly based on shape returned from AI gym. \n",
        "    init_screen = get_screen(env)\n",
        "    _, _, screen_height, screen_width = init_screen.shape\n",
        "    print(\"screen size: \", screen_height, screen_width)\n",
        "\n",
        "    # Are we resuming from an existing model?\n",
        "    policy_net = None\n",
        "    if load_filename is not None and os.path.isfile(os.path.join(MODEL_PATH, load_filename)):\n",
        "        print(\"Loading model...\")\n",
        "        policy_net = load_model(load_filename)\n",
        "        policy_net = policy_net.to(DEVICE)\n",
        "        print(\"Done loading.\")\n",
        "    else:\n",
        "        print(\"Making new model.\")\n",
        "        policy_net = DQN(screen_height, screen_width, env.NUM_ACTIONS).to(DEVICE)\n",
        "    # Make a copy of the policy network for evaluation purposes\n",
        "    eval_net = DQN(screen_height, screen_width, env.NUM_ACTIONS).to(DEVICE)\n",
        "    eval_net.load_state_dict(policy_net.state_dict())\n",
        "    eval_net.eval()\n",
        "    # Target network is a snapshot of the policy network that lags behind (for stablity)\n",
        "    target_net = DQN(screen_height, screen_width, env.NUM_ACTIONS).to(DEVICE)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "    \n",
        "    # Instantiate the optimizer\n",
        "    optimizer = None\n",
        "    if len(list(policy_net.parameters())) > 0:\n",
        "        optimizer = initializeOptimizer(policy_net.parameters())\n",
        "    \n",
        "    # Instantiate the replay memory\n",
        "    replay_memory = ReplayMemory(replay_capacity)\n",
        "\n",
        "    steps_done = 0               # How many steps have been run\n",
        "    best_eval = float('inf')     # The best model evaluation to date\n",
        "\n",
        "    ### Do training until episodes complete \n",
        "    print(\"training...\")\n",
        "    i_episode = 0            # The episode number\n",
        "    \n",
        "    # Stop when we reach max episodes\n",
        "    while i_episode < num_episodes:\n",
        "        print(\"episode:\", i_episode, \"epsilon:\", epsilon)\n",
        "        max_reward = 0       # The best reward we've seen this episode\n",
        "        done = False         # Has the game ended (timed out or got the coin)\n",
        "        episode_steps = 0    # Number of steps performed in this episode\n",
        "        # Initialize the environment and state\n",
        "        env.reset()\n",
        "        \n",
        "        # Current screen. There is no last screen because we get velocity on the screen itself.\n",
        "        state = get_screen(env)\n",
        "\n",
        "        # Do forever until the loop breaks \n",
        "        while not done:\n",
        "            # Select and perform an action\n",
        "            action, epsilon = select_action(state, policy_net, env.NUM_ACTIONS, epsilon, steps_done, bootstrap_threshold)\n",
        "            steps_done = steps_done + 1\n",
        "            episode_steps = episode_steps + 1\n",
        "            \n",
        "            # for debugging\n",
        "            if RENDER_SCREEN and not IN_PYNB:\n",
        "                env.render() \n",
        "\n",
        "            # Run the action in the environment\n",
        "            if action is not None: \n",
        "                _, reward, done, _ = env.step(np.array([action.item()]))\n",
        "\n",
        "                # Record if this was the best reward we've seen so far\n",
        "                max_reward = max(reward, max_reward)\n",
        "                \n",
        "                # Turn the reward into a tensor  \n",
        "                reward = torch.tensor([reward], device=DEVICE)\n",
        "\n",
        "                # Observe new state\n",
        "                current_screen = get_screen(env)\n",
        "\n",
        "                # Did the game end?\n",
        "                if not done:\n",
        "                    next_state = current_screen\n",
        "                else:\n",
        "                    next_state = None\n",
        "\n",
        "                # Store the transition in memory\n",
        "                replay_memory.push(state, action, next_state, reward)\n",
        "\n",
        "                # Move to the next state\n",
        "                state = next_state\n",
        "\n",
        "                # If we are past bootstrapping we should perform one step of the optimization\n",
        "                if steps_done > bootstrap_threshold:\n",
        "                  optimize_model(policy_net, target_net if target_update > 0 else policy_net, replay_memory, optimizer, batch_size, gamma)\n",
        "            else:\n",
        "                # Do nothing if select_action() is not implemented and returning None\n",
        "                env.step(np.array([0]))\n",
        "                \n",
        "            # If we are done, print some statistics\n",
        "            if done:\n",
        "                print(\"duration:\", episode_steps)\n",
        "                print(\"max reward:\", max_reward)\n",
        "                status, _ = episode_status(episode_steps, max_reward)\n",
        "                print(\"result:\", status)\n",
        "                print(\"total steps:\", steps_done, '\\n')\n",
        "\n",
        "            # Should we update the target network?\n",
        "            if target_update > 0 and i_episode % target_update == 0:\n",
        "                target_net.load_state_dict(policy_net.state_dict())\n",
        "                \n",
        "        # Should we evaluate?\n",
        "        if steps_done > bootstrap_threshold and i_episode > 0 and i_episode % eval_interval == 0:\n",
        "            test_average_duration = 0       # Track the average eval duration\n",
        "            test_average_max_reward = 0     # Track the average max reward\n",
        "            # copy all the weights into the evaluation network\n",
        "            eval_net.load_state_dict(policy_net.state_dict())\n",
        "            # Evaluate 10 times\n",
        "            for _ in range(EVAL_COUNT):\n",
        "                # Call the evaluation function\n",
        "                test_duration, test_max_reward = evaluate(eval_net, eval_epsilon, env, test_seed=seed)\n",
        "                status, score = episode_status(test_duration, test_max_reward)\n",
        "                test_duration = score # Set test_duration to score to factor in death-penalty\n",
        "                test_average_duration = test_average_duration + test_duration\n",
        "                test_average_max_reward = test_average_max_reward + test_max_reward\n",
        "            test_average_duration = test_average_duration / EVAL_COUNT\n",
        "            test_average_max_reward = test_average_max_reward / EVAL_COUNT\n",
        "            print(\"Average duration:\", test_average_duration)\n",
        "            print(\"Average max reward:\", test_average_max_reward)\n",
        "            # If this is the best window average we've seen, save the model\n",
        "            if test_average_duration < best_eval:\n",
        "                best_eval = test_average_duration\n",
        "                if save_filename is not None:\n",
        "                    save_model(policy_net, save_filename, i_episode)\n",
        "            print(' ')\n",
        "        # Only increment episode number if we are done with bootstrapping\n",
        "        if steps_done > bootstrap_threshold:\n",
        "          i_episode = i_episode + 1\n",
        "    print('Training complete')\n",
        "    if RENDER_SCREEN and not IN_PYNB:\n",
        "        env.render()\n",
        "    env.close()\n",
        "    return policy_net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCkpDANAURJT"
      },
      "source": [
        "\n",
        "## Optimization Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "7l6qXukQUVyk"
      },
      "outputs": [],
      "source": [
        "\n",
        "### Take a DQN and do one forward-backward pass.\n",
        "### Since this is Q-learning, we will run a forward pass to get Q-values for state-action pairs and then \n",
        "### give the true value as the Q-values after the Q-update equation.\n",
        "def optimize_model(policy_net, target_net, replay_memory, optimizer, batch_size, gamma):\n",
        "    if len(replay_memory) < batch_size:\n",
        "        return\n",
        "    ### step 1: sample from the replay memory. Get BATCH_SIZE transitions\n",
        "    ### Step 2: Get a list of non-final next states.\n",
        "    ###         a. Create a mask, a tensor of length BATCH_SIZE where each element i is 1 if \n",
        "    ###            batch.next_state[i] is not None and 0 otherwise.\n",
        "    ###         b. Create a tensor of shape [BATCH_SIZE, color(3), height, width] by concatenating\n",
        "    ###            all non-final (not None) batch.next_states together.\n",
        "    ### Step 3: set up batches for state, action, and reward\n",
        "    ###         a. Create a tensor of shape [BATCH_SIZE, color(3), height, width] holding states\n",
        "    ###         b. Create a tensor of shape [BATCH_SIZE, 1] holding actions\n",
        "    ###         c. Create a tensor of shape [BATCH_SIZE, 1] holding rewards\n",
        "    states_batch, actions_batch, next_states_batch, rewards_batch, non_final_mask = doMakeBatch(replay_memory, batch_size)\n",
        "\n",
        "    ### Step 4: Get the action values predicted.\n",
        "    ###         a. Call policy_net(state_batch) to get a tensor of shape [BATCH_SIZE, NUM_ACTIONS] containing Q-values\n",
        "    ###         b. For each batch, get the Q-value for the corresponding action in action_batch (hint: torch.gather)\n",
        "    state_action_values = doPredictQValues(policy_net, states_batch, actions_batch)\n",
        "\n",
        "    ### Step 5: Get the utility values of next_states.\n",
        "    next_state_values = doPredictNextStateUtilities(target_net, next_states_batch, non_final_mask, batch_size)\n",
        "    \n",
        "    ### Step 6: Compute the expected Q values.\n",
        "    expected_state_action_values = doComputeExpectedQValues(next_state_values, rewards_batch, gamma)\n",
        "\n",
        "    ### Step 7: Computer Huber loss (smooth L1 loss)\n",
        "    ###         Compare state action values from step 5 to expected state action values from step 7\n",
        "    loss = doComputeLoss(state_action_values, expected_state_action_values)\n",
        "    ### Step 8: Back propagation\n",
        "    ###         a. Zero out gradients\n",
        "    ###         b. call loss.backward()\n",
        "    ###         c. Prevent gradient explosion by clipping gradients between -1 and 1\n",
        "    ###            (hint: param.grad.data is the gradients. See torch.clamp_() )\n",
        "    ###         d. Tell the optimizer that another step has occurred: optimizer.step()\n",
        "    if optimizer is not None:\n",
        "        optimizer.zero_grad()\n",
        "        doBackprop(loss, policy_net.parameters())\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXuPMZw7UZla"
      },
      "source": [
        "## Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "DGy1g-scUbX3"
      },
      "outputs": [],
      "source": [
        "\n",
        "### Evaluate the DQN\n",
        "### If environment is given, use that. Otherwise make a new environment.\n",
        "def evaluate(policy_net, epsilon = EVAL_EPSILON, env = None, test_seed = SEED):\n",
        "    global RUN_NUM\n",
        "    RUN_NUM = RUN_NUM + 1\n",
        "    setup_utils.setup_and_load(use_cmd_line_args=False, is_high_res=True, num_levels=NUM_LEVELS, set_seed=test_seed)\n",
        "    \n",
        "\n",
        "    # Make an environment if we don't already have one\n",
        "    if env is None:\n",
        "        env = make('standard', num_envs=1)\n",
        "    if RENDER_SCREEN and not IN_PYNB:\n",
        "        env.render()\n",
        "\n",
        "    # Reset the environment\n",
        "    env.reset()\n",
        "\n",
        "    # Get screen size so that we can initialize layers correctly based on shape\n",
        "    # returned from AI gym. \n",
        "    init_screen = get_screen(env, SCREEN_SAVE)\n",
        "    _, _, screen_height, screen_width = init_screen.shape\n",
        "\n",
        "    # Get the network ready for evaluation (turns off some things like dropout if used)\n",
        "    policy_net.eval()\n",
        "\n",
        "    # Current screen. There is no last screen\n",
        "    state = get_screen(env, SCREEN_SAVE)\n",
        "\n",
        "    steps_done = 0         # Number of steps executed\n",
        "    max_reward = 0         # Max reward seen\n",
        "    done = False           # Is the game over?\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    while not done:\n",
        "        # Select and perform an action\n",
        "        action, _ = select_action(state, policy_net, env.NUM_ACTIONS, epsilon, steps_done=0, bootstrap_threshold=0)\n",
        "        steps_done = steps_done + 1\n",
        "\n",
        "        if RENDER_SCREEN and not IN_PYNB:\n",
        "            env.render()          \n",
        "\n",
        "        # Execute the action\n",
        "        if action is not None:\n",
        "            _, reward, done, _ = env.step(np.array([action.item()]))\n",
        "\n",
        "            # Is this the best reward we've seen?\n",
        "            max_reward = max(reward, max_reward)\n",
        "\n",
        "            # Observe new state\n",
        "            state = get_screen(env, SCREEN_SAVE)\n",
        "        else:\n",
        "            # Do nothing if select_action() is not implemented and returning None\n",
        "            env.step(np.array([0]))\n",
        "\n",
        "    print(\"duration:\", steps_done)\n",
        "    print(\"max reward:\", max_reward)\n",
        "    status, _ = episode_status(steps_done, max_reward)\n",
        "    print(\"result:\", status, '\\n')\n",
        "    if RENDER_SCREEN and not IN_PYNB:\n",
        "        env.render()\n",
        "    return steps_done, max_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVXdyG9ZRfnu"
      },
      "source": [
        "## DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "r3SKJfm-Ek6Z"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    ### Create all the nodes in the computation graph.\n",
        "    ### We won't say how to put the nodes together into a computation graph. That is done\n",
        "    ### automatically when forward() is called.\n",
        "    def __init__(self, h, w, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # Conv -> Conv -> Conv -> Linear -> Linear\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.batchNorm1 = nn.BatchNorm2d(16)\n",
        "        h_out = h - 5 // 2 * 2\n",
        "        w_out = w - 5 // 2 * 2\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.batchNorm2 = nn.BatchNorm2d(32)\n",
        "        h_out = h_out - 5 // 2 * 2\n",
        "        w_out = w_out - 5 // 2 * 2\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.batchNorm3 = nn.BatchNorm2d(32)\n",
        "        h_out = (h_out - 5) // 2 + 1\n",
        "        w_out = (w_out - 5) // 2 + 1\n",
        "\n",
        "        linear_in = 32*h_out*w_out\n",
        "        linear_out = 1024\n",
        "\n",
        "        self.linear1 = nn.Linear(linear_in, linear_out)\n",
        "        self.linearRelu = nn.ReLU()\n",
        "        \n",
        "        self.linear2 = nn.Linear(linear_out, num_actions)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        q_values = None\n",
        "        t = self.batchNorm1(self.relu1(self.conv1(x)))\n",
        "        t = self.batchNorm2(self.relu2(self.conv2(t)))\n",
        "        t = self.batchNorm3(self.relu3(self.conv3(t)))\n",
        "        #flatten out image for linear layer\n",
        "        t = torch.flatten(t, start_dim=1)\n",
        "        t = self.linearRelu(self.linear1(t))\n",
        "\n",
        "        q_values = self.linear2(t)\n",
        "                 \n",
        "        return q_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNKfO3pQRlrx"
      },
      "source": [
        "## get_screen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "Z3RlDdQigCx0"
      },
      "outputs": [],
      "source": [
        "### Take the environment and return a tensor containing screen data as a 3D tensor containing (color, height, width) information.\n",
        "def get_screen(env, save = False):\n",
        "    # Returned screen requested by gym is 512x512x3. Transpose it into torch order (Color, Height, Width).\n",
        "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    # Convert to float, rescale, convert to torch tensor\n",
        "    # (this doesn't require a copy)\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    if save:\n",
        "      save_screen(screen)\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # Resize, and add a batch dimension (BCHW)\n",
        "    return resize(screen).unsqueeze(0).to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ospeXs0qRppd"
      },
      "source": [
        "## ReplayMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "YqLyc5EggFeT"
      },
      "outputs": [],
      "source": [
        "### Store transitions to use to prevent catastrophic forgetting.\n",
        "### ReplayMemory implements a ring buffer. Items are placed into memory\n",
        "###    until memory reaches capacity, and then new items start replacing old items\n",
        "###    at the beginning of the array. \n",
        "### Member variables:\n",
        "###    capacity: (int) number of transitions that can be stored\n",
        "###    memory: (array) holds transitions (state, action, next_state, reward)\n",
        "###    position: (int) index of current location in memory to place the next transition.\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    ### Store a transition in memory.\n",
        "    ### To implement: put new items at the end of the memory array, unless capacity is reached.\n",
        "    ###    Combine the arguments into a new Transition object.\n",
        "    ###    If capacity is reached, start overwriting the beginning of the array.\n",
        "    ###    Use the position index to keep track of where to put the next item. \n",
        "    def push(self, state, action, next_state, reward):\n",
        "        \n",
        "        new_mem = Transition(state, action, next_state, reward)\n",
        "\n",
        "        if self.position >= self.capacity:\n",
        "          self.position = 0\n",
        "        \n",
        "        if len(self.memory) < self.capacity:\n",
        "          self.memory.append(new_mem)\n",
        "\n",
        "        else:\n",
        "          self.memory[self.position] = new_mem\n",
        "        \n",
        "        self.position += 1\n",
        "        \n",
        "    ### Return a batch of transition objects from memory containing batch_size elements.\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    ### This allows one to call len() on a ReplayMemory object. E.g. len(replay_memory)\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xskf9M0_RurS"
      },
      "source": [
        "## initializeOptimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "8C17xRYCRrKc"
      },
      "outputs": [],
      "source": [
        "### Choose and instantiate an optimizer\n",
        "### Input:\n",
        "### - parameters: the DQN parameters\n",
        "### Output:\n",
        "### - the optimizer object\n",
        "def initializeOptimizer(parameters):\n",
        "    optimizer = optim.Adam(parameters, lr=0.00005)\n",
        "    #optimizer = optim.SGD(parameters, lr=0.001)\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVVcvZO8RzAC"
      },
      "source": [
        "## select_action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "Fje2XRRYgYUp"
      },
      "outputs": [],
      "source": [
        "### Select an action to perform. \n",
        "### If a random number [0..1] is greater than epsilon, then query the policy_network,\n",
        "### otherwise use a random action.\n",
        "### Inputs:\n",
        "### - state: a tensor of shape 3 x screen_height x screen_width\n",
        "### - policy_net: a DQN object\n",
        "### - num_actions: number of actions available\n",
        "### - epsilon: float [0..1] indicating whether to choose random or use the network\n",
        "### - steps_done: number of previously executed steps\n",
        "### - bootstrap_threshold: number of steps that must be executed before training begins\n",
        "### This function should return:\n",
        "### - A tensor of shape 1 x 1 that contains the number of the action to execute\n",
        "### - The new epsilon value to use next time\n",
        "def select_action(state, policy_net, num_actions, epsilon, steps_done = 0, bootstrap_threshold = 0):\n",
        "    global EPSILON_DECAY_NUM\n",
        "\n",
        "    # state_device = state.to(DEVICE)\n",
        "    random_num = torch.rand(1)\n",
        "\n",
        "    if random_num > epsilon:\n",
        "      # choose policy\n",
        "      with torch.no_grad():\n",
        "        action = torch.argmax(policy_net(state)).unsqueeze(0).unsqueeze(0)\n",
        "    else:\n",
        "      #random action\n",
        "      action = torch.randint(low=0, high=num_actions, size=(1,1)).to(DEVICE)\n",
        "      # action.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    if steps_done > bootstrap_threshold:\n",
        "      # update epsilon\n",
        "      new_epsilon = 0.1 + (EPSILON - 0.1) * math.exp(-1. * EPSILON_DECAY_NUM / 50000)\n",
        "      EPSILON_DECAY_NUM += 1\n",
        "      if new_epsilon < 0.1:\n",
        "        new_epsilon = 0.1\n",
        "    \n",
        "    return action, new_epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgKYPlpJR5yt"
      },
      "source": [
        "## doMakeBatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "Mjnp4s9RgbxZ"
      },
      "outputs": [],
      "source": [
        "### Ask for a batch of experience replays.\n",
        "### Inputs:\n",
        "### - replay_memory: A ReplayMemory object\n",
        "### - batch_size: size of the batch to return\n",
        "### Outputs:\n",
        "### - states_batch: a tensor of shape batch_size x 3 x screen_height x screen_width\n",
        "### - actions_batch: a tensor of shape batch_size x 1 containing action numbers\n",
        "### - next_states_batch: a tensor containing screens. \n",
        "### - rewards_batch: a tensor of shape batch_size x 1 containing reward values.\n",
        "### - non_final_mask: a tensor of bytes of length batch_size containing a 0 if the state is terminal or 1 otherwise\n",
        "def doMakeBatch(replay_memory, batch_size):\n",
        "    batch = replay_memory.sample(batch_size)\n",
        "    batch = zip(*batch)\n",
        "    # print(\"batch should be (state,state,state), (action,action,action), ...\")\n",
        "    # print(batch)\n",
        "    # print(len(batch))\n",
        "\n",
        "    states, actions, next, rewards = batch\n",
        "\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, next)), device=DEVICE, dtype=torch.bool)\n",
        "    \n",
        "    next_states_batch = torch.cat([s for s in next if s is not None]).to(DEVICE)\n",
        "\n",
        "    states_batch = torch.cat(states, dim=0).to(DEVICE)\n",
        "    actions_batch = torch.cat(actions, dim=0).to(DEVICE)\n",
        "    rewards_batch = torch.cat(rewards, dim=0).to(DEVICE)\n",
        "\n",
        "    # print(\"states_batch: \", states_batch.shape)\n",
        "    # print(\"actions_batch: \", actions_batch.shape)\n",
        "    # print(\"next_states_batch: \", next_states_batch.shape)\n",
        "    # print(\"rewards_batch: \", rewards_batch.shape)\n",
        "    # print(\"non_final_mask: \", non_final_mask.shape)\n",
        "\n",
        "    return states_batch, actions_batch, next_states_batch, rewards_batch, non_final_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuhTG2aZR8wJ"
      },
      "source": [
        "## doPredictQValues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "ZLulwgrAgcjB"
      },
      "outputs": [],
      "source": [
        "### Ask the policy_net to predict the Q value for a batch of states and a batch of actions.\n",
        "### Inputs:\n",
        "### - policy_net: the DQN\n",
        "### - states_batch: a tensor of shape batch_size x 3 x screen_height x screen_width containing screens\n",
        "### - actions_batch: a tensor of shape batch_size x 1 containing action numbers\n",
        "### Output:\n",
        "### - A tensor of shape batch_size x 1 containing the Q-value predicted by the DQN in the position indicated by the action\n",
        "def doPredictQValues(policy_net, states_batch, actions_batch):\n",
        "\n",
        "    state_action_values = policy_net(states_batch).gather(1, actions_batch)\n",
        "\n",
        "    return state_action_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVJ5HgVQSNgq"
      },
      "source": [
        "## doPredictNextStateUtilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "Em2AfgD1gecz"
      },
      "outputs": [],
      "source": [
        "### Ask the policy_net to predict the utility of a next_state.\n",
        "### Inputs:\n",
        "### - policy_net: The DQN\n",
        "### - next_states_batch: a tensor of shape batch_size x 3 x screen_height x screen_width\n",
        "### - non_final_mask: a tensor of length batch_size containing 0 for terminal states and 1 for non-terminal states\n",
        "### - batch_size: the batch size\n",
        "### Note: Only run non-terminal states through the policy_net\n",
        "### Output:\n",
        "### - A tensor of shape batch_size x 1 containing Q-values\n",
        "def doPredictNextStateUtilities(policy_net, next_states_batch, non_final_mask, batch_size):\n",
        "    next_state_values = torch.zeros(batch_size, device=DEVICE).float()\n",
        "\n",
        "    if next_states_batch.shape[0] != torch.count_nonzero(non_final_mask):\n",
        "\n",
        "      next_states_batch = next_states_batch[non_final_mask].float()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      next_state_values[non_final_mask] = torch.argmax(policy_net(next_states_batch), dim=1).float()\n",
        "\n",
        "    next_state_values = next_state_values.unsqueeze(1)\n",
        "    \n",
        "    return next_state_values.detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9605CTaSQhR"
      },
      "source": [
        "## doComputeExpectedQValues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "ES5pukXuggdX"
      },
      "outputs": [],
      "source": [
        "### Compute the Q-update equation Q(s_t, a_t) = R(s_t+1) + gamma * argmax_a' Q(s_t+1, a')\n",
        "### Inputs:\n",
        "### - next_state_values: a tensor of shape batch_size x 1 containing Q values for state s_t+1\n",
        "### - rewards_batch: a tensor or shape batch_size x 1 containing reward values for state s_t+1\n",
        "### Output:\n",
        "### - A tensor of shape batch_size x 1\n",
        "def doComputeExpectedQValues(next_state_values, rewards_batch, gamma):\n",
        "    expected_state_action_values = rewards_batch + gamma * next_state_values\n",
        "\n",
        "    return expected_state_action_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3l9RwRlSS-b"
      },
      "source": [
        "## doComputeLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "KeePwmIDgja-"
      },
      "outputs": [],
      "source": [
        "### Compute the loss\n",
        "### Inputs:\n",
        "### - state_action_values: a tensor of shape batch_size x 1 containing Q values\n",
        "### - expected_state_action_values: a tensor of shape batch_size x 1 containing updated Q values\n",
        "### Output:\n",
        "### - A tensor scalar value\n",
        "def doComputeLoss(state_action_values, expected_state_action_values):\n",
        "\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHsR5t-qSVBk"
      },
      "source": [
        "## doBackprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "qKoDTe_ChEWd"
      },
      "outputs": [],
      "source": [
        "### Run backpropagation. Make sure gradients are clipped between -1 and +1.\n",
        "### Inputs:\n",
        "### - loss: a tensor scalar\n",
        "### - parameters: the parameters of the DQN\n",
        "### There is no output\n",
        "def doBackprop(loss, parameters):\n",
        "    loss.backward()\n",
        "    # print(type(parameters))\n",
        "    # print(parameters)\n",
        "    for p in parameters:\n",
        "      p.grad = torch.clamp(p.grad, -1, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KivfSnMYHdT7"
      },
      "source": [
        "# Unit testing\n",
        "\n",
        "Run this cell to make sure your functions return valid outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNWu5e1zoc8v",
        "outputId": "d89ec892-0908-45cf-ab8d-f29241536259"
      },
      "outputs": [],
      "source": [
        "### RUN UNIT TESTS ON FUNCTIONS\n",
        "unit_test() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5twAqf6HhT8"
      },
      "source": [
        "# Training\n",
        "\n",
        "Edit the globals as necessary.\n",
        "The best model produced during training will be saved to ```SAVE_FILENAME``` with a number appended to the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vkAqkDanQa6"
      },
      "source": [
        "###### Train for EASY_LEVEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rVeaxxTLZEXp",
        "outputId": "d2a5a3c4-b5b9-4a0b-ab17-eea18d958bd5"
      },
      "outputs": [],
      "source": [
        "SAVE_FILENAME = 'easy.model'\n",
        "LOAD_FILENAME = None\n",
        "TRAIN_SEED = EASY_LEVEL\n",
        "policy_net = train(num_episodes=NUM_EPISODES, \n",
        "                   load_filename=LOAD_FILENAME, \n",
        "                   save_filename=SAVE_FILENAME, \n",
        "                   eval_interval=EVAL_INTERVAL, \n",
        "                   replay_capacity=REPLAY_CAPACITY, \n",
        "                   bootstrap_threshold=BOOTSTRAP, \n",
        "                   target_update=TARGET_UPDATE,\n",
        "                   epsilon=EPSILON, \n",
        "                   eval_epsilon=EVAL_EPSILON,\n",
        "                   gamma=GAMMA, \n",
        "                   batch_size=BATCH_SIZE,\n",
        "                   random_seed=RANDOM_SEED,\n",
        "                   seed=TRAIN_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7dInZrecSid",
        "outputId": "250ee5c1-31a3-4352-beef-27bdfbb28666"
      },
      "outputs": [],
      "source": [
        "print(EPSILON_DECAY_NUM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALs1mZNUnYM3"
      },
      "source": [
        "Train for MEDIUM_LEVEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiQi6LpzbJZe",
        "outputId": "306ef0a2-e25c-4570-be9c-f1757e150f9e"
      },
      "outputs": [],
      "source": [
        "SAVE_FILENAME = 'medium.model'\n",
        "LOAD_FILENAME = None\n",
        "TRAIN_SEED = MEDIUM_LEVEL\n",
        "policy_net = train(num_episodes=NUM_EPISODES, \n",
        "                   load_filename=LOAD_FILENAME, \n",
        "                   save_filename=SAVE_FILENAME, \n",
        "                   eval_interval=EVAL_INTERVAL, \n",
        "                   replay_capacity=REPLAY_CAPACITY, \n",
        "                   bootstrap_threshold=BOOTSTRAP, \n",
        "                   target_update=TARGET_UPDATE,\n",
        "                   epsilon=EPSILON, \n",
        "                   eval_epsilon=EVAL_EPSILON,\n",
        "                   gamma=GAMMA, \n",
        "                   batch_size=BATCH_SIZE,\n",
        "                   random_seed=RANDOM_SEED,\n",
        "                   seed=TRAIN_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQnvWYw6nbNs"
      },
      "source": [
        "Train for ONE_MONSTER Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20M6QJ90bWzs",
        "outputId": "29b5ecf0-932c-4e24-e40a-6e440a37b0cc"
      },
      "outputs": [],
      "source": [
        "SAVE_FILENAME = 'monster.model'\n",
        "LOAD_FILENAME = None\n",
        "TRAIN_SEED = ONE_MONSTER\n",
        "policy_net = train(num_episodes=NUM_EPISODES, \n",
        "                   load_filename=LOAD_FILENAME, \n",
        "                   save_filename=SAVE_FILENAME, \n",
        "                   eval_interval=EVAL_INTERVAL, \n",
        "                   replay_capacity=REPLAY_CAPACITY, \n",
        "                   bootstrap_threshold=BOOTSTRAP, \n",
        "                   target_update=TARGET_UPDATE,\n",
        "                   epsilon=EPSILON, \n",
        "                   eval_epsilon=EVAL_EPSILON,\n",
        "                   gamma=GAMMA, \n",
        "                   batch_size=BATCH_SIZE,\n",
        "                   random_seed=RANDOM_SEED,\n",
        "                   seed=TRAIN_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksX662zTHmq1"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "This loads the saved model (even if there is a number appended to the end) and evaluates it.\n",
        "\n",
        "It also produces a dump of screenshots for each run of ```evaluate()``` in ```/content/cache/```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaQzdCi8nfs3"
      },
      "source": [
        "Evaluate the easy.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1NEZBIlj3IC",
        "outputId": "4930913d-905f-4edc-ac14-7d028c40a60a"
      },
      "outputs": [],
      "source": [
        "TEST_FILENAME = 'easy.model'\n",
        "TEST_SEED = EASY_LEVEL\n",
        "eval_net = load_model(TEST_FILENAME)\n",
        "\n",
        "for _ in range(EVAL_COUNT):\n",
        "  duration, max_reward = evaluate(eval_net, epsilon=EVAL_EPSILON, test_seed=TEST_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1es9x93nkJ-"
      },
      "source": [
        " Evaluate the medium.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5RAP5k4k-4o",
        "outputId": "fc538617-d82b-43c6-cdcb-f7d60b38553c"
      },
      "outputs": [],
      "source": [
        "TEST_FILENAME = 'medium.model'\n",
        "TEST_SEED = MEDIUM_LEVEL\n",
        "eval_net = load_model(TEST_FILENAME)\n",
        "\n",
        "for _ in range(EVAL_COUNT):\n",
        "  duration, max_reward = evaluate(eval_net, epsilon=EVAL_EPSILON, test_seed=TEST_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYKBYHZNnl1b"
      },
      "source": [
        "Evaluate the monster.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEugWNGhlDyR",
        "outputId": "6db24d17-5d37-4786-fa1f-96664731e89f"
      },
      "outputs": [],
      "source": [
        "TEST_FILENAME = 'monster.model'\n",
        "TEST_SEED = ONE_MONSTER\n",
        "eval_net = load_model(TEST_FILENAME)\n",
        "\n",
        "for _ in range(EVAL_COUNT):\n",
        "  duration, max_reward = evaluate(eval_net, epsilon=EVAL_EPSILON, test_seed=TEST_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Easy Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-LNqy8SVoJx",
        "outputId": "89e828b0-5799-488e-c0fc-b43b674d4cb5"
      },
      "outputs": [],
      "source": [
        "TEST_FILENAME = 'easy.model'\n",
        "TEST_SEED = EASY_LEVEL\n",
        "eval_net = load_model(TEST_FILENAME)\n",
        "best_easy_duration = None\n",
        "best_easy_grade = 0.0\n",
        "\n",
        "try:\n",
        "  for i in range(10):\n",
        "    print(\"EASY LEVEL TRY\", i)\n",
        "    easy_grade = 0.0\n",
        "    total_duration = 0.0\n",
        "    for j in range(10):\n",
        "      duration, max_reward = evaluate(eval_net, epsilon = EVAL_EPSILON, test_seed = TEST_SEED)\n",
        "      status, score = episode_status(duration, max_reward)\n",
        "      total_duration = total_duration + score\n",
        "    average_duration = total_duration / 10.0\n",
        "    if average_duration < 150:\n",
        "      easy_grade = easy_grade + 3\n",
        "    if average_duration < 100:\n",
        "      easy_grade = easy_grade + 1\n",
        "    if average_duration < 50:\n",
        "      easy_grade = easy_grade + 1\n",
        "    if easy_grade > best_easy_grade:\n",
        "      best_easy_grade = easy_grade\n",
        "    if best_easy_duration is None or average_duration < best_easy_duration:\n",
        "      best_easy_duration = average_duration\n",
        "    print(\"EASY SCORE\", i, easy_grade, average_duration)\n",
        "    if easy_grade >= 5.0:\n",
        "      break\n",
        "except Exception as e: \n",
        "  print(\"EASY GRADING BROKE\")\n",
        "\n",
        "print(\"BEST EASY SCORE\", best_easy_grade, best_easy_duration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq0uNKnSnqLX"
      },
      "source": [
        "Medium Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lX31CQrNcBh_",
        "outputId": "fa0fc7ac-d6af-43fe-ef0c-c9e75ff53545"
      },
      "outputs": [],
      "source": [
        "TEST_FILENAME = 'medium.model'\n",
        "TEST_SEED = MEDIUM_LEVEL\n",
        "eval_net = load_model(TEST_FILENAME)\n",
        "best_medium_duration = None\n",
        "best_medium_grade = 0.0\n",
        "\n",
        "try:\n",
        "  for i in range(10):\n",
        "    print(\"MEDIUM LEVEL TRY\", i)\n",
        "    medium_grade = 0.0\n",
        "    total_duration = 0.0\n",
        "    for j in range(10):\n",
        "      duration, max_reward = evaluate(eval_net, epsilon = EVAL_EPSILON, test_seed = TEST_SEED)\n",
        "      status, score = episode_status(duration, max_reward)\n",
        "      total_duration = total_duration + score\n",
        "    average_duration = total_duration / 10.0\n",
        "    if average_duration < 150:\n",
        "      medium_grade = medium_grade + 1\n",
        "    if medium_grade > best_medium_grade:\n",
        "      best_medium_grade = medium_grade\n",
        "    if best_medium_duration is None or average_duration < best_medium_duration:\n",
        "      best_medium_duration = average_duration\n",
        "    print(\"MEDIUM SCORE\", i, medium_grade, average_duration)\n",
        "    if medium_grade >= 1.0:\n",
        "      break\n",
        "except Exception as e: \n",
        "  print(\"MEDIUM GRADING BROKE\")\n",
        "\n",
        "print(\"BEST MEDIUM SCORE\", best_medium_grade, best_medium_duration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdzsgwmmnr2S"
      },
      "source": [
        "Monster (Hard) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkhVr7D0cK61",
        "outputId": "92dc46f6-389a-4f9d-f31c-4ac56161af28"
      },
      "outputs": [],
      "source": [
        "TEST_FILENAME = 'monster.model'\n",
        "TEST_SEED = ONE_MONSTER\n",
        "eval_net = load_model(TEST_FILENAME)\n",
        "best_monster_duration = None\n",
        "best_monster_grade = 0.0\n",
        "\n",
        "try:\n",
        "  for i in range(10):\n",
        "    print(\"MONSTER LEVEL TRY\", i)\n",
        "    monster_grade = 0.0\n",
        "    total_duration = 0.0\n",
        "    for j in range(10):\n",
        "      duration, max_reward = evaluate(eval_net, epsilon = EVAL_EPSILON, test_seed = TEST_SEED)\n",
        "      status, score = episode_status(duration, max_reward)\n",
        "      total_duration = total_duration + score\n",
        "    average_duration = total_duration / 10.0\n",
        "    if average_duration < 300:\n",
        "      monster_grade = monster_grade + 1\n",
        "    if monster_grade > best_monster_grade:\n",
        "      best_monster_grade = monster_grade\n",
        "    if best_monster_duration is None or average_duration < best_monster_duration:\n",
        "      best_monster_duration = average_duration\n",
        "    print(\"MONSTER SCORE\", i, monster_grade, average_duration)\n",
        "    if monster_grade >= 1.0:\n",
        "      break\n",
        "except Exception as e: \n",
        "  print(\"MONSTER GRADING BROKE\")\n",
        "\n",
        "print(\"BEST MONSTER SCORE\", best_monster_grade, best_monster_duration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwsTL1mJHpPu"
      },
      "source": [
        "# Download The Model\n",
        "\n",
        "If you want to download the model to a computer without NVIDIA GPU and CUDA support, you can use the cell below. It takes the model indicated, converts it to run on CPU, and saves it. You can then open the file browser to the left and download the model file.<br/> <br/><b>Change the model name in load_model argument below to download medium and monster models respectively.</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwWE-O3MYNVy"
      },
      "outputs": [],
      "source": [
        "### USE THIS TO COVERT A MODEL FROM CUDA TO CPU\n",
        "### DOWNLOAD USING THE FILES MENU TO THE LEFT\n",
        "\n",
        "### CHANGE THE MODEL NAME BELOW FOR MEDIUM AND MONSTER MODELS\n",
        "net = load_model('easy.model')\n",
        "net = net.to('cpu')\n",
        "torch.save(net, os.path.join(MODEL_PATH, 'saved_cpu.model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ4XRSaksJxE"
      },
      "source": [
        "# Show Evaluation Movies\n",
        "\n",
        "The following cells will render an animation in the notebook. It is flickery but you should get an idea of what your agent was duing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQjdIOubsZVE"
      },
      "outputs": [],
      "source": [
        "# This seems to work but is really slow and flickery\n",
        "\n",
        "eval_run_number = 1\n",
        "show_movie(eval_run_number)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avR87kPlsv46"
      },
      "source": [
        "Alternatively, the following makes an animated gif. The animation is faster and doesn't flicker once loaded. Unfortunately, colab loads animated gifs really slow.\n",
        "Better to download and view on one's own machine (though this is cumbersome)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnqqHTBotBQX"
      },
      "outputs": [],
      "source": [
        "eval_run_number = 1\n",
        "make_anim(eval_run_number)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
